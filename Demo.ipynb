{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The below set up is for a Google Colab notebook. This assumes that this notebook is in a Google Folder called TMaze, which contains all the files in [the Github repository](https://github.com/annikaheuser/TMaze/blob/main/tmaze.py)."
   ],
   "metadata": {
    "id": "lKF9ArsbKD-H"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Added March 2024: Ensuring stable compatibility between CUDA (11.8), numpy (1.23.1), torch (2.2.0), and mxnet (mxnet-cu117).\n",
    "\n",
    "# Downgrade CUDA to 11.8\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
    "!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu1804-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
    "!dpkg -i cuda-repo-ubuntu1804-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
    "!cp /var/cuda-repo-ubuntu1804-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "!apt-get update\n",
    "!apt-get -y install cuda-11-8\n",
    "\n",
    "# Downgrade numpy to handle np.bool issue\n",
    "# (will require restarting runtime after execution)\n",
    "!pip install numpy==1.23.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Installations\n",
    "!ln -s /usr/local/cuda/lib64/libcusolver.so.11 /usr/local/cuda/lib64/libcusolver.so.10\n",
    "!ls /usr/local/cuda/lib64/libcusolver*\n",
    "!pip install torch==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install mxnet-cu117\n",
    "!pip install wordfreq language_tool_python transformers\n",
    "!git clone https://github.com/awslabs/mlm-scoring.git mlm_scoring/\n",
    "!cd mlm_scoring/; git checkout 9cab61e6774bcc4983f7117f1a280c334f3e68b5; sed -i '21s/.*/\"transformers\",/' setup.py; cat setup.py; pip install .; pip install .; cd .."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezR2LagIi_K9",
    "outputId": "161dcf2e-4a0a-4b3a-9d31-0f0876bc67d1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R5uZvELfNFz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "50111924-aec0-4930-9a86-47c6d9f3b790"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "import torch\n",
    "import mxnet as mx\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "WORK_PATH = \"/content/gdrive/My Drive/TMaze\"\n",
    "from mlm.scorers import MLMScorer, MLMScorerPT, LMScorer\n",
    "from mlm.models import get_pretrained\n",
    "import mxnet as mx\n",
    "import pickle\n",
    "import wordfreq\n",
    "import string\n",
    "from scipy.stats import norm\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(WORK_PATH)\n",
    "import tmaze\n",
    "import materials\n",
    "import ibex_prep\n",
    "import lang_spec"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "id": "qbgoH8_4i8eL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b253f9cf-09f8-4ce4-a567-254a0bfbdca5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "RyU4iUr7KZ_w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're using a language other than English, please refer to [Heuser, 2022](https://dspace.mit.edu/handle/1721.1/147233) for detailed instructions on how to create the necessary language-specific files. In English, these are `nonwords_en.pkl` and `freq_bins_en_ensemble.pkl`, which were created via functions in `lang_spec.py` and are included in [the Github repository](https://github.com/annikaheuser/TMaze/blob/main/tmaze.py).\n",
    "\n"
   ],
   "metadata": {
    "id": "bsEM1Q2jto4E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eng = lang_spec.lang_spec(\"en-US\",True,WORK_PATH)\n",
    "eng.compile_freq_bins_and_nonwords_set()"
   ],
   "metadata": {
    "id": "ZmbzvrgCO6Aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e37c93d2-66a8-4637-d84f-7f4f09d20294"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`boyce_materials_formatted.txt` has the expected format for experimental materials that are to be matched with distractors, namely:\n",
    "```\n",
    "ConditionName;ItemID;Sentence\n",
    "```\n",
    "For example:\n",
    "\n",
    "```\n",
    "adverb_high;72;Kim will display the photos she took next month, but she won't show all of them.\n",
    "adverb_low;72;Kim will display the photos she took last month, but she won't show all of them.\n",
    "```\n",
    "This file was derived from [g_maze.js](https://github.com/vboyce/Maze/blob/master/experiment/Materials/g_maze.js), made by Boyce et al. (2020).\n"
   ],
   "metadata": {
    "id": "dyhyag5Uwtvc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(f'{WORK_PATH}/boyce_materials_formatted.txt') as f:\n",
    "    sents = f.readlines()"
   ],
   "metadata": {
    "id": "3bxqZ0fPfOoc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The materials object has a number of potentially useful attributes. Refer to [materials.py](https://github.com/annikaheuser/TMaze/blob/main/materials.py) for all of them. The following code creates new files with the names specified in the dictorionary at `WORK_PATH/{file_name}`. For example, in this case we will create `/content/gdrive/My Drive/TMaze/BoyceCondDict.pkl`."
   ],
   "metadata": {
    "id": "k4R173BWTEJ1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "m_pickle_dict = {\"cond_dict\": \"BoyceCondDict.pkl\", \"word_info\": \"BoyceWordInfo.pkl\",\"item_pairs\": \"BoyceNumItemPairs.pkl\"}\n",
    "boyce = materials.materials(sents,\";\",WORK_PATH,'en',m_pickle_dict)"
   ],
   "metadata": {
    "id": "6HI-uWmyfcEG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f6c23497-377c-402d-e032-a942169838a1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we specify the transformer model that TMaze should use to produce materials. Run\n",
    "\n",
    "```\n",
    "mlm.models.SUPPORTED_MLMS\n",
    "```\n",
    "to see what other models can be run by just changing the string in the below code. 'bert-base-multi-cased' may work decently well for languages like German, French, or Spanish.\n"
   ],
   "metadata": {
    "id": "cfDp5OKaU7BD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ctxs = [mx.gpu(0)]\n",
    "model, vocab, tokenizer = get_pretrained(ctxs, 'bert-base-en-uncased')\n",
    "scorer = MLMScorer(model, vocab, tokenizer, ctxs)"
   ],
   "metadata": {
    "id": "eZtI7riN4veu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "efdd9f31-b4e5-448a-ac48-bb9605fb52f4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pickle_dict = {\"freq_dict\":f'{WORK_PATH}/freq_bins_en_ensemble.pkl', \"word_info\": f\"{WORK_PATH}/BoyceWordInfo.pkl\", \"nonwords_set\": f\"{WORK_PATH}/nonwords_en.pkl\", \"dists_dict\":f\"{WORK_PATH}/EnglishDistractors.pkl\"}\n",
    "with open(pickle_dict['dists_dict'], \"wb\") as f:\n",
    "  pickle.dump({},f) #to initialize the dictionary within TMaze"
   ],
   "metadata": {
    "id": "hnVkcif6iaWZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above pickle files are either the result of the language specific setup (which are included in the Github repository for English) or from loading in the materials (i.e. `materials.materials(...)`). We initialize the final one at the desired file path in the code block above.\n",
    "\n",
    "TMaze takes the scorer (which assigns strings psuedologlikelihood values or log likelihood values based on a model) as its first argument. While the scorer does not necessarily need to be from `mlm.scorers`, it does need a `score_sentences` function to be compatible with the current code. Therefore you may need to build a simple object based on a transformer such that it returns a list of likelihood values for each string `score_sentences` is passed. See the [mlm-scoring repository](https://github.com/awslabs/mlm-scoring) from [Salazar et al. (2020)](https://aclanthology.org/2020.acl-main.240/) for more details.\n",
    "\n",
    "The second argument is the name of a [spaCy](https://spacy.io/) pipeline. The available pipelines are listed here: https://spacy.io/models. This was introduced for part of speech tagging purposes, which we keep track of for analysis purposes. These are included in a dataframe with all the generated distractors."
   ],
   "metadata": {
    "id": "StXWJnzaouwg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "en_tmaze = tmaze.tmaze(scorer,'en_core_web_sm',WORK_PATH,pickle_dict)"
   ],
   "metadata": {
    "id": "kils6yyP4sXG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next code block produces and saves the resulting experimental materials as a Javascript file so that it can easily be plugged in to PCIbex.\n",
    "\n",
    "`ibex_prep.compile_all_sent_items_from_dict` actually determines the distractors. It takes two objects built earlier in this notebook: the TMaze object (i.e. `en_tmaze`) and an attribute of the loaded experimental materials.\n",
    "\n",
    "The third parameter is the number of potential distractors checked before returning the best one for any given words. In other words, with this set to 100, we find the best of 100 potential distractors. Increasing this number increases the time the function will take to run but the distractors it then returns might be higher quality.\n",
    "\n",
    "The last parameter is the number of top distractors that are saved in the `pandas` dataframe returned by the function. In this case, we save the top 3 distractors for every word. If the chosen distractor is unideal for any reason, then we can replace it with the second or even third best distractor. We save the dataframe in a csv, in case we want to reload it after generating our distractors for any reason.\n",
    "\n",
    "In addition to the dataframe, the function returns the sentences in JavaScript (JS) formatting, which we then write to a JS file. This file can quickly be plugged into a PCIbex project. We uploaded [this Github repository](https://github.com/vboyce/Ibex-with-Maze) to PCIbex and then inserted the content of `boyce_matchedDistractors.js` into the `sample.js` file for our validation experiment in [Heuser, 2022](https://dspace.mit.edu/handle/1721.1/147233)."
   ],
   "metadata": {
    "id": "q-rrmH-Bosto"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "items_js, dist_df = ibex_prep.compile_all_sent_items_from_dict(en_tmaze,boyce.num_item_pairs,100,3)\n",
    "js_to_write = \"\"\n",
    "js_to_write+=items_js\n",
    "with open(f\"{WORK_PATH}/boyce_matchedDistractors.js\",\"w\") as doc:\n",
    "  doc.write(js_to_write)\n",
    "dist_df.to_csv(f\"{WORK_PATH}/boyce100matchedDistractors.csv\")"
   ],
   "metadata": {
    "id": "423_SHDSirK1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31134495-48bc-4323-b3e9-268e3b0bd5a8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following commented out functions were written to allow you to easily adjust the language specific setup to your experimental materials. `delete_nonwords_after` adds words to the list of nonwords that should not be considered for distractors, such as acronyms or slang words, for example, if your experimental materials consist of formal language. `switch_word_cap` allows you to make a word that might have been algorithmically saved as lowercase, like \"trump's,\" uppercase because it is more commonly found in this form, and vice versa."
   ],
   "metadata": {
    "id": "ZRxC5iwWIeHH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#eng.delete_nonwords_after([\"werid_word0\",\"weird_word1\"])\n",
    "#eng.switch_word_cap(\"Wrong_capitalized\",\"wrong_lowercase\")"
   ],
   "metadata": {
    "id": "DKC-tpaFM_Gi"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
